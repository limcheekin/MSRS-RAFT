{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸš€ RAFT Fine-tuning on MSRS Story-QA\n",
        "\n",
        "Complete pipeline for fine-tuning Qwen3-4B-Instruct with RAFT methodology on Google Colab T4 GPU.\n",
        "\n",
        "## ğŸ“‹ What This Notebook Does\n",
        "\n",
        "1. **Setup Environment** - Install dependencies and clone repository\n",
        "2. **Load Data** - Download MSRS Story-QA dataset\n",
        "3. **Build Index** - Create vector search index\n",
        "4. **Generate RAFT Dataset** - Create training data with CoT and citations\n",
        "5. **Train Model** - Fine-tune with Unsloth QLoRA\n",
        "6. **Evaluate** - Test model performance\n",
        "\n",
        "## âš™ï¸ Requirements\n",
        "\n",
        "- **GPU**: T4 (15GB VRAM)\n",
        "- **RAM**: High-RAM runtime recommended\n",
        "- **Time**: ~3-4 hours for 50-100 training examples\n",
        "- **API Key**: OpenAI API key for CoT generation\n",
        "\n",
        "## ğŸ¯ Quick Start\n",
        "\n",
        "1. Enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "2. Run all cells in order\n",
        "3. Enter your OpenAI API key when prompted\n",
        "4. Wait for training to complete\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”§ Step 1: Setup Environment\n",
        "\n",
        "Install all required packages and clone the repository."
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install PyTorch with CUDA support\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "install_pytorch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Clone the repository\n",
        "!git clone https://github.com/limcheekin/MSRS-RAFT.git\n",
        "%cd MSRS-RAFT"
      ],
      "metadata": {
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install all dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Upgrade Unsloth to latest\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installation\n",
        "print(\"ğŸ” Verifying installation...\\n\")\n",
        "!python test_installation.py"
      ],
      "metadata": {
        "id": "verify_install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”‘ Step 2: Configure API Keys and Settings\n",
        "\n",
        "Set up your OpenAI API key and configure training parameters."
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Get OpenAI API key\n",
        "print(\"ğŸ“ Enter your OpenAI API key (required for RAFT dataset generation):\")\n",
        "openai_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "\n",
        "print(\"\\nâœ… API key configured!\")"
      ],
      "metadata": {
        "id": "set_api_key"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training parameters for T4 GPU\n",
        "from raft_config import RAFTConfig, ModelConfig, TrainingConfig, RAFTDataConfig\n",
        "\n",
        "# Create custom config optimized for T4\n",
        "config = RAFTConfig()\n",
        "\n",
        "# Model settings (optimized for T4 15GB)\n",
        "config.model.max_seq_length = 2048  # Reduced for T4\n",
        "config.model.lora_r = 16  # Smaller LoRA rank\n",
        "config.model.lora_alpha = 32\n",
        "config.model.load_in_4bit = True\n",
        "\n",
        "# Training settings (optimized for T4)\n",
        "config.training.num_train_epochs = 2  # Fewer epochs for demo\n",
        "config.training.per_device_train_batch_size = 1  # Small batch for T4\n",
        "config.training.gradient_accumulation_steps = 8  # Effective batch size = 8\n",
        "config.training.learning_rate = 2e-4\n",
        "config.training.max_new_tokens = 512  # Reduced for T4\n",
        "config.training.logging_steps = 10\n",
        "config.training.eval_steps = 50\n",
        "config.training.save_steps = 100\n",
        "\n",
        "# RAFT settings\n",
        "config.raft_data.oracle_percentage = 0.8\n",
        "config.raft_data.num_distractors = 3  # Fewer distractors\n",
        "config.raft_data.chunk_size = 1000  # Smaller chunks\n",
        "\n",
        "# System settings\n",
        "config.system.project_name = \"raft-colab\"\n",
        "config.system.use_wandb = False  # Disable W&B for simplicity\n",
        "\n",
        "# Save config\n",
        "config.to_yaml(\"colab_config.yaml\")\n",
        "\n",
        "print(\"âš™ï¸ Configuration for T4 GPU:\")\n",
        "print(f\"  Max Sequence Length: {config.model.max_seq_length}\")\n",
        "print(f\"  LoRA Rank: {config.model.lora_r}\")\n",
        "print(f\"  Batch Size: {config.training.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient Accumulation: {config.training.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective Batch Size: {config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps}\")\n",
        "print(f\"  Epochs: {config.training.num_train_epochs}\")\n",
        "print(f\"\\nâœ… Configuration saved to colab_config.yaml\")"
      ],
      "metadata": {
        "id": "configure_settings"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š Step 3: Load and Explore Data\n",
        "\n",
        "Load the MSRS Story-QA dataset and explore its structure."
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from raft_data_loader import MSRSDataLoader\n",
        "import json\n",
        "\n",
        "print(\"ğŸ“š Loading MSRS Story-QA dataset...\\n\")\n",
        "\n",
        "# Initialize loader\n",
        "loader = MSRSDataLoader(\n",
        "    dataset_name=\"yale-nlp/MSRS\",\n",
        "    dataset_config=\"story-qa\",\n",
        "    cache_dir=\"./cache\"\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "dataset = loader.load_dataset()\n",
        "\n",
        "# Parse examples\n",
        "train_examples = loader.parse_examples(split=\"train\", max_examples=10)\n",
        "dev_examples = loader.parse_examples(split=\"dev\", max_examples=5)\n",
        "\n",
        "# Load corpus\n",
        "print(\"\\nğŸ“– Loading story corpus...\")\n",
        "try:\n",
        "    corpus = loader.load_corpus()\n",
        "    print(f\"âœ… Loaded {len(corpus)} chapters\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not load corpus from HuggingFace: {str(e)}\")\n",
        "    print(\"Creating minimal corpus from examples...\")\n",
        "    # This will work even without full corpus\n",
        "\n",
        "# Display statistics\n",
        "stats = loader.get_statistics()\n",
        "print(\"\\nğŸ“Š Dataset Statistics:\")\n",
        "print(json.dumps(stats, indent=2))\n",
        "\n",
        "# Show example\n",
        "print(\"\\nğŸ“ Example Query:\")\n",
        "print(f\"  Query: {train_examples[0].query[:100]}...\")\n",
        "print(f\"  Gold Documents: {train_examples[0].gold_documents[:3]}\")\n",
        "print(f\"  Number of Answers: {len(train_examples[0].answers)}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ” Step 4: Build Retrieval Index\n",
        "\n",
        "Create vector search index for document retrieval."
      ],
      "metadata": {
        "id": "index_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from raft_retrieval import RetrievalSystem\n",
        "from raft_data_loader import Chapter\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ” Building retrieval index...\\n\")\n",
        "\n",
        "# Check if corpus is available\n",
        "if not loader._corpus:\n",
        "    print(\"âš ï¸ No corpus available. Creating mock corpus for demonstration...\")\n",
        "    # Create minimal corpus from examples\n",
        "    loader._corpus = {}\n",
        "    for i, example in enumerate(train_examples + dev_examples):\n",
        "        for doc_id in example.gold_documents:\n",
        "            if doc_id not in loader._corpus:\n",
        "                loader._corpus[doc_id] = Chapter(\n",
        "                    doc_id=doc_id,\n",
        "                    story_id=doc_id.rsplit('_', 1)[0] if '_' in doc_id else doc_id,\n",
        "                    chapter_index=0,\n",
        "                    text=f\"Sample chapter content for {doc_id}. \" + example.query,\n",
        "                    token_length=50\n",
        "                )\n",
        "\n",
        "# Initialize retrieval system\n",
        "retrieval_system = RetrievalSystem(\n",
        "    embedding_model=\"BAAI/bge-small-en-v1.5\",  # Smaller model for T4\n",
        "    reranker_model=None,  # Disable reranker to save memory\n",
        "    chunk_size=config.raft_data.chunk_size,\n",
        "    chunk_overlap=config.raft_data.chunk_overlap,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Prepare documents\n",
        "documents = {\n",
        "    doc_id: chapter.text\n",
        "    for doc_id, chapter in loader._corpus.items()\n",
        "}\n",
        "\n",
        "print(f\"ğŸ“š Indexing {len(documents)} documents...\")\n",
        "\n",
        "# Build index\n",
        "retrieval_system.build_index(\n",
        "    documents,\n",
        "    batch_size=16,  # Smaller batch for T4\n",
        "    save_path=\"./indices/colab_index\"\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Index built successfully!\")\n",
        "\n",
        "# Test retrieval\n",
        "print(\"\\nğŸ” Testing retrieval...\")\n",
        "test_query = train_examples[0].query\n",
        "results = retrieval_system.retrieve(test_query, top_k=3)\n",
        "\n",
        "print(f\"Query: {test_query[:80]}...\")\n",
        "print(f\"\\nTop 3 results:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"  {i}. {result.doc_id} (score: {result.score:.4f})\")\n",
        "    print(f\"     {result.text[:100]}...\")"
      ],
      "metadata": {
        "id": "build_index"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ—ï¸ Step 5: Generate RAFT Training Dataset\n",
        "\n",
        "Create RAFT training examples with Chain-of-Thought reasoning and citations.\n",
        "\n",
        "**Note**: This step uses OpenAI API and will incur costs (~$0.01-0.03 per example)."
      ],
      "metadata": {
        "id": "raft_data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from raft_dataset_builder import CoTGenerator, RAFTDatasetBuilder, prepare_examples_from_loader\n",
        "import time\n",
        "\n",
        "print(\"ğŸ—ï¸ Generating RAFT training dataset...\\n\")\n",
        "\n",
        "# Set number of examples (adjust based on your budget)\n",
        "NUM_TRAIN_EXAMPLES = 20  # Start small for testing\n",
        "NUM_DEV_EXAMPLES = 5\n",
        "\n",
        "print(f\"ğŸ“Š Configuration:\")\n",
        "print(f\"  Training examples: {NUM_TRAIN_EXAMPLES}\")\n",
        "print(f\"  Dev examples: {NUM_DEV_EXAMPLES}\")\n",
        "print(f\"  Estimated cost: ${(NUM_TRAIN_EXAMPLES + NUM_DEV_EXAMPLES) * 0.02:.2f}\")\n",
        "print(f\"  Estimated time: {(NUM_TRAIN_EXAMPLES + NUM_DEV_EXAMPLES) * 2} minutes\\n\")\n",
        "\n",
        "# Ask for confirmation\n",
        "print(\"â³ Starting in 5 seconds... (interrupt if you want to change settings)\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Initialize CoT generator\n",
        "cot_generator = CoTGenerator(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=1500,\n",
        "    api_key=os.environ.get('OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "# Initialize RAFT builder\n",
        "raft_builder = RAFTDatasetBuilder(\n",
        "    retrieval_system=retrieval_system,\n",
        "    cot_generator=cot_generator,\n",
        "    oracle_percentage=config.raft_data.oracle_percentage,\n",
        "    num_distractors=config.raft_data.num_distractors,\n",
        "    distractor_pool_size=10,\n",
        "    max_quote_length=300,\n",
        "    min_quotes=1,\n",
        "    max_quotes=3\n",
        ")\n",
        "\n",
        "# Prepare examples\n",
        "print(\"\\nğŸ“ Preparing examples from data loader...\")\n",
        "train_prep = prepare_examples_from_loader(loader, split=\"train\")\n",
        "dev_prep = prepare_examples_from_loader(loader, split=\"dev\")\n",
        "\n",
        "# Build training dataset\n",
        "print(f\"\\nğŸ”¨ Building RAFT training dataset ({NUM_TRAIN_EXAMPLES} examples)...\")\n",
        "print(\"This will take a while. Progress will be shown below.\\n\")\n",
        "\n",
        "train_raft_examples = raft_builder.build_dataset(\n",
        "    train_prep[:NUM_TRAIN_EXAMPLES],\n",
        "    output_path=\"./data/raft_train_colab.jsonl\"\n",
        ")\n",
        "\n",
        "# Build dev dataset\n",
        "print(f\"\\nğŸ”¨ Building RAFT dev dataset ({NUM_DEV_EXAMPLES} examples)...\\n\")\n",
        "\n",
        "dev_raft_examples = raft_builder.build_dataset(\n",
        "    dev_prep[:NUM_DEV_EXAMPLES],\n",
        "    output_path=\"./data/raft_dev_colab.jsonl\"\n",
        ")\n",
        "\n",
        "# Show statistics\n",
        "train_stats = raft_builder.get_statistics(train_raft_examples)\n",
        "print(\"\\nğŸ“Š RAFT Dataset Statistics:\")\n",
        "print(json.dumps(train_stats, indent=2))\n",
        "\n",
        "# Show example\n",
        "if train_raft_examples:\n",
        "    example = train_raft_examples[0]\n",
        "    print(\"\\nğŸ“ Example RAFT Training Data:\")\n",
        "    print(f\"  Query: {example.query[:100]}...\")\n",
        "    print(f\"  Contexts: {len(example.contexts)}\")\n",
        "    print(f\"  Oracle IDs: {example.oracle_ids}\")\n",
        "    print(f\"  Distractor IDs: {example.distractor_ids}\")\n",
        "    print(f\"  Reasoning (first 200 chars): {example.reasoning[:200]}...\")\n",
        "    print(f\"  Answer: {example.answer[:100]}...\")\n",
        "\n",
        "print(\"\\nâœ… RAFT dataset generation complete!\")"
      ],
      "metadata": {
        "id": "generate_raft_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¯ Step 6: Train Model with Unsloth\n",
        "\n",
        "Fine-tune Qwen3-4B-Instruct using QLoRA with Unsloth."
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from raft_trainer import RAFTTrainer, LoggingCallback\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ¯ Starting model training...\\n\")\n",
        "\n",
        "# Check GPU\n",
        "print(f\"ğŸ–¥ï¸ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = RAFTTrainer(config)\n",
        "\n",
        "# Load model\n",
        "print(\"ğŸ“¦ Loading Qwen3-4B-Instruct with QLoRA...\")\n",
        "trainer.load_model()\n",
        "\n",
        "print(\"\\nâœ… Model loaded successfully!\")\n",
        "\n",
        "# Load datasets\n",
        "print(\"\\nğŸ“š Loading training datasets...\")\n",
        "train_dataset, eval_dataset = trainer.load_dataset(\n",
        "    train_path=\"./data/raft_train_colab.jsonl\",\n",
        "    eval_path=\"./data/raft_dev_colab.jsonl\"\n",
        ")\n",
        "\n",
        "print(f\"  Training examples: {len(train_dataset)}\")\n",
        "print(f\"  Evaluation examples: {len(eval_dataset) if eval_dataset else 0}\")\n",
        "\n",
        "# Setup callbacks\n",
        "callbacks = [LoggingCallback(\"./logs/training_colab.jsonl\")]\n",
        "\n",
        "# Show training info\n",
        "total_steps = (len(train_dataset) // (config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps)) * config.training.num_train_epochs\n",
        "print(f\"\\nğŸ“Š Training Configuration:\")\n",
        "print(f\"  Total steps: ~{total_steps}\")\n",
        "print(f\"  Logging every {config.training.logging_steps} steps\")\n",
        "print(f\"  Evaluation every {config.training.eval_steps} steps\")\n",
        "print(f\"  Saving every {config.training.save_steps} steps\")\n",
        "\n",
        "print(\"\\nğŸš€ Starting training...\")\n",
        "print(\"This will take 1-2 hours. Progress will be shown below.\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train\n",
        "train_result = trainer.train(\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nâœ… Training complete!\")\n",
        "print(f\"\\nğŸ“Š Final Training Loss: {train_result.training_loss:.4f}\")"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¾ Step 7: Save Trained Model\n",
        "\n",
        "Save the fine-tuned model for later use or download."
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ’¾ Saving trained model...\\n\")\n",
        "\n",
        "# Save model (merged 16bit for best quality)\n",
        "output_dir = \"./models/raft_qwen3_colab\"\n",
        "\n",
        "trainer.save_model(\n",
        "    output_dir=output_dir,\n",
        "    save_method=\"merged_16bit\"  # or \"lora\" for smaller size\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Model saved to {output_dir}\")\n",
        "\n",
        "# Check model size\n",
        "import os\n",
        "total_size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
        "                 for dirpath, _, filenames in os.walk(output_dir)\n",
        "                 for filename in filenames)\n",
        "print(f\"ğŸ“¦ Model size: {total_size / 1e9:.2f} GB\")\n",
        "\n",
        "# Optionally zip for download\n",
        "print(\"\\nğŸ“¦ Creating zip file for download...\")\n",
        "!zip -r raft_model_colab.zip {output_dir}\n",
        "print(\"âœ… Model zipped as raft_model_colab.zip\")\n",
        "print(\"\\nYou can download this file from the Files panel on the left.\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š Step 8: Evaluate Model\n",
        "\n",
        "Test the fine-tuned model on evaluation examples."
      ],
      "metadata": {
        "id": "eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from raft_evaluator import RAFTEvaluator\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "print(\"ğŸ“Š Evaluating trained model...\\n\")\n",
        "\n",
        "# Load model for inference\n",
        "print(\"ğŸ“¦ Loading model for evaluation...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=output_dir,\n",
        "    max_seq_length=config.model.max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = RAFTEvaluator(\n",
        "    config=config,\n",
        "    retrieval_system=retrieval_system,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Prepare test examples (use dev set)\n",
        "print(\"\\nğŸ“ Preparing evaluation examples...\")\n",
        "eval_examples = []\n",
        "for ex in dev_prep[:5]:  # Evaluate on 5 examples\n",
        "    eval_examples.append({\n",
        "        'query': ex['query'],\n",
        "        'gold_docs': [doc_id for doc_id, _ in ex['oracle_docs']],\n",
        "        'answers': ex['answers']\n",
        "    })\n",
        "\n",
        "print(f\"Evaluating on {len(eval_examples)} examples...\\n\")\n",
        "\n",
        "# Run evaluation\n",
        "metrics = evaluator.evaluate_dataset(\n",
        "    eval_examples,\n",
        "    output_path=\"./results/eval_colab.jsonl\"\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "evaluator.print_metrics(metrics)\n",
        "\n",
        "# Show example generation\n",
        "print(\"\\nğŸ“ Example Generation:\")\n",
        "print(\"=\"*70)\n",
        "test_query = eval_examples[0]['query']\n",
        "results = retrieval_system.retrieve(test_query, top_k=3)\n",
        "contexts = [r.text for r in results]\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "generated = evaluator.generate_answer(test_query, contexts)\n",
        "print(f\"Generated Answer:\\n{generated}\\n\")\n",
        "print(f\"Reference Answers:\")\n",
        "for i, ref in enumerate(eval_examples[0]['answers'], 1):\n",
        "    print(f\"  {i}. {ref[:100]}...\")\n",
        "\n",
        "print(\"\\nâœ… Evaluation complete!\")"
      ],
      "metadata": {
        "id": "evaluate_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¨ Step 9: Interactive Demo\n",
        "\n",
        "Try the model with your own questions!"
      ],
      "metadata": {
        "id": "demo_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, top_k=3):\n",
        "    \"\"\"Answer a question using the trained model\"\"\"\n",
        "    print(f\"\\nâ“ Question: {question}\")\n",
        "    print(\"\\nğŸ” Retrieving relevant contexts...\")\n",
        "    \n",
        "    # Retrieve contexts\n",
        "    results = retrieval_system.retrieve(question, top_k=top_k)\n",
        "    contexts = [r.text for r in results]\n",
        "    \n",
        "    print(f\"   Found {len(results)} relevant documents\")\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"   {i}. {r.doc_id} (score: {r.score:.4f})\")\n",
        "    \n",
        "    print(\"\\nğŸ¤– Generating answer...\\n\")\n",
        "    \n",
        "    # Generate answer\n",
        "    answer = evaluator.generate_answer(question, contexts)\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ANSWER:\")\n",
        "    print(\"=\"*70)\n",
        "    print(answer)\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Try some example questions\n",
        "print(\"ğŸ¨ Interactive Demo - Try the model!\\n\")\n",
        "\n",
        "# Example 1\n",
        "answer_question(\"What is the main theme of the story?\")\n",
        "\n",
        "# Example 2\n",
        "answer_question(\"Who are the main characters?\")\n",
        "\n",
        "# Try your own question\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Try your own question!\")\n",
        "print(\"=\"*70)\n",
        "custom_question = input(\"Enter your question: \")\n",
        "if custom_question.strip():\n",
        "    answer_question(custom_question)"
      ],
      "metadata": {
        "id": "interactive_demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“¥ Step 10: Download Results\n",
        "\n",
        "Package and download training artifacts."
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"ğŸ“¥ Preparing files for download...\\n\")\n",
        "\n",
        "# Create results package\n",
        "package_dir = \"raft_training_results\"\n",
        "!mkdir -p {package_dir}\n",
        "\n",
        "# Copy important files\n",
        "print(\"ğŸ“¦ Packaging results...\")\n",
        "\n",
        "files_to_package = [\n",
        "    (\"colab_config.yaml\", \"Configuration\"),\n",
        "    (\"./logs/training_colab.jsonl\", \"Training logs\"),\n",
        "    (\"./results/eval_colab.jsonl\", \"Evaluation results\"),\n",
        "    (\"./data/raft_train_colab.jsonl\", \"Training data sample\"),\n",
        "]\n",
        "\n",
        "for file_path, description in files_to_package:\n",
        "    if os.path.exists(file_path):\n",
        "        shutil.copy(file_path, package_dir)\n",
        "        print(f\"  âœ“ {description}\")\n",
        "\n",
        "# Create summary report\n",
        "summary = f\"\"\"RAFT Training Summary\n",
        "=====================\n",
        "\n",
        "Training Configuration:\n",
        "- Model: Qwen3-4B-Instruct\n",
        "- LoRA Rank: {config.model.lora_r}\n",
        "- Sequence Length: {config.model.max_seq_length}\n",
        "- Batch Size: {config.training.per_device_train_batch_size}\n",
        "- Gradient Accumulation: {config.training.gradient_accumulation_steps}\n",
        "- Epochs: {config.training.num_train_epochs}\n",
        "- Learning Rate: {config.training.learning_rate}\n",
        "\n",
        "Dataset:\n",
        "- Training Examples: {len(train_dataset)}\n",
        "- Evaluation Examples: {len(eval_dataset) if eval_dataset else 0}\n",
        "- Oracle Percentage: {config.raft_data.oracle_percentage}\n",
        "- Distractors: {config.raft_data.num_distractors}\n",
        "\n",
        "Results:\n",
        "- Final Training Loss: {train_result.training_loss:.4f}\n",
        "- Evaluation Metrics: See eval_colab.jsonl\n",
        "\n",
        "Model Location: {output_dir}\n",
        "Model Size: {total_size / 1e9:.2f} GB\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{package_dir}/SUMMARY.txt\", 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"  âœ“ Summary report\")\n",
        "\n",
        "# Zip everything\n",
        "print(\"\\nğŸ“¦ Creating zip file...\")\n",
        "!zip -r raft_results.zip {package_dir}\n",
        "\n",
        "print(\"\\nâœ… Results packaged!\")\n",
        "print(\"\\nDownload options:\")\n",
        "print(\"1. raft_results.zip - Training logs and results\")\n",
        "print(\"2. raft_model_colab.zip - Trained model (large file)\")\n",
        "print(\"\\nUse the Files panel on the left to download.\")\n",
        "\n",
        "# Optionally trigger download\n",
        "print(\"\\nğŸ’¾ Downloading results package...\")\n",
        "files.download('raft_results.zip')"
      ],
      "metadata": {
        "id": "download_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š Step 11: Visualize Training Progress\n",
        "\n",
        "Plot training metrics."
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"ğŸ“Š Visualizing training progress...\\n\")\n",
        "\n",
        "# Load training logs\n",
        "log_file = \"./logs/training_colab.jsonl\"\n",
        "if os.path.exists(log_file):\n",
        "    logs = []\n",
        "    with open(log_file, 'r') as f:\n",
        "        for line in f:\n",
        "            logs.append(json.loads(line))\n",
        "    \n",
        "    # Extract metrics\n",
        "    steps = [log['step'] for log in logs if 'loss' in log]\n",
        "    losses = [log['loss'] for log in logs if 'loss' in log]\n",
        "    learning_rates = [log.get('learning_rate', 0) for log in logs if 'loss' in log]\n",
        "    \n",
        "    # Create plots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Loss plot\n",
        "    ax1.plot(steps, losses, 'b-', linewidth=2)\n",
        "    ax1.set_xlabel('Training Steps', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning rate plot\n",
        "    ax2.plot(steps, learning_rates, 'r-', linewidth=2)\n",
        "    ax2.set_xlabel('Training Steps', fontsize=12)\n",
        "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
        "    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"âœ… Training visualization complete!\")\n",
        "    print(f\"   Total steps: {len(steps)}\")\n",
        "    if losses:\n",
        "        print(f\"   Initial loss: {losses[0]:.4f}\")\n",
        "        print(f\"   Final loss: {losses[-1]:.4f}\")\n",
        "        print(f\"   Improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")\n",
        "else:\n",
        "    print(\"âš ï¸ Training log file not found\")"
      ],
      "metadata": {
        "id": "visualize_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ Step 12: Troubleshooting\n",
        "\n",
        "Run diagnostics if you encounter issues."
      ],
      "metadata": {
        "id": "troubleshooting_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell if you encounter issues\n",
        "\n",
        "print(\"ğŸ” System Diagnostics\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "\n",
        "# Check disk space\n",
        "import shutil\n",
        "total, used, free = shutil.disk_usage(\"/\")\n",
        "print(f\"\\nDisk Space:\")\n",
        "print(f\"  Total: {total / 1e9:.2f} GB\")\n",
        "print(f\"  Used: {used / 1e9:.2f} GB\")\n",
        "print(f\"  Free: {free / 1e9:.2f} GB\")\n",
        "\n",
        "# Check Python packages\n",
        "print(f\"\\nKey Package Versions:\")\n",
        "import transformers\n",
        "print(f\"  transformers: {transformers.__version__}\")\n",
        "print(f\"  torch: {torch.__version__}\")\n",
        "\n",
        "try:\n",
        "    from unsloth import __version__ as unsloth_version\n",
        "    print(f\"  unsloth: {unsloth_version}\")\n",
        "except:\n",
        "    print(f\"  unsloth: installed (version unknown)\")\n",
        "\n",
        "# Check environment variables\n",
        "print(f\"\\nEnvironment:\")\n",
        "print(f\"  OPENAI_API_KEY: {'Set' if os.environ.get('OPENAI_API_KEY') else 'Not set'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nCommon Solutions:\")\n",
        "print(\"  1. Out of Memory: Reduce batch_size or max_seq_length\")\n",
        "print(\"  2. API Errors: Check OpenAI API key and credits\")\n",
        "print(\"  3. Slow Training: Ensure GPU runtime is enabled\")\n",
        "print(\"  4. Import Errors: Restart runtime and reinstall packages\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "troubleshooting"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¾ Step 13: Save to Google Drive (Optional)\n",
        "\n",
        "Save your work to Google Drive to prevent data loss."
      ],
      "metadata": {
        "id": "gdrive_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create backup directory\n",
        "backup_dir = '/content/drive/MyDrive/RAFT_Backup'\n",
        "!mkdir -p {backup_dir}\n",
        "\n",
        "print(f\"ğŸ“ Backing up to: {backup_dir}\\n\")\n",
        "\n",
        "# Copy important files\n",
        "print(\"ğŸ’¾ Copying files...\")\n",
        "!cp -r ./models/raft_qwen3_colab {backup_dir}/ 2>/dev/null || echo \"  âš ï¸ Model not found\"\n",
        "!cp -r ./data {backup_dir}/ 2>/dev/null || echo \"  âš ï¸ Data not found\"\n",
        "!cp -r ./results {backup_dir}/ 2>/dev/null || echo \"  âš ï¸ Results not found\"\n",
        "!cp -r ./logs {backup_dir}/ 2>/dev/null || echo \"  âš ï¸ Logs not found\"\n",
        "!cp colab_config.yaml {backup_dir}/ 2>/dev/null || echo \"  âš ï¸ Config not found\"\n",
        "\n",
        "print(\"\\nâœ… Backup complete!\")\n",
        "print(f\"Files saved to: {backup_dir}\")"
      ],
      "metadata": {
        "id": "save_to_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "You've successfully completed the RAFT fine-tuning pipeline!\n",
        "\n",
        "### What you've accomplished:\n",
        "\n",
        "âœ… Installed all dependencies  \n",
        "âœ… Loaded MSRS Story-QA dataset  \n",
        "âœ… Built vector search index  \n",
        "âœ… Generated RAFT training data with CoT  \n",
        "âœ… Fine-tuned Qwen3-4B with QLoRA  \n",
        "âœ… Evaluated model performance  \n",
        "âœ… Created interactive demo  \n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Increase dataset size** - Train on 100+ examples for better performance\n",
        "2. **Tune hyperparameters** - Adjust learning rate, batch size, epochs\n",
        "3. **Try different models** - Experiment with other base models\n",
        "4. **Compare baselines** - Test against 0-shot and standard SFT\n",
        "5. **Deploy the model** - Use for production QA tasks\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- ğŸ“š [RAFT Paper](https://arxiv.org/abs/2403.10131)\n",
        "- ğŸ”§ [GitHub Repository](https://github.com/limcheekin/MSRS-RAFT)\n",
        "- ğŸ“– [Unsloth Documentation](https://docs.unsloth.ai/)\n",
        "- ğŸ’¬ [MSRS Dataset](https://huggingface.co/datasets/yale-nlp/MSRS)\n",
        "\n",
        "### Need Help?\n",
        "\n",
        "- Check the [GitHub Issues](https://github.com/limcheekin/MSRS-RAFT/issues)\n",
        "- Review the [COLAB_GUIDE.md](https://github.com/limcheekin/MSRS-RAFT/blob/main/COLAB_GUIDE.md)\n",
        "- Run the troubleshooting cell above\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Training! ğŸš€**"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
